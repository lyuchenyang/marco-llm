---
layout: default
title: Multilingual LLM Project
---

<section id="introduction">
    <h1>Marco-LLM: Bridging Languages via Massive
Multilingual Training for Cross-Lingual Enhancement</h1>  
<style>
.highlight-blue { background-color: #cce5ff; }
.highlight-green { background-color: #d4edda; }
.highlight-red { background-color: #f8d7da; }
.highlight-yellow { background-color: #fff3cd; }
</style>

<p>
Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages.
To address this issue, we introduced Marco-LLM: <span class="highlight-blue">Ma</span>ssive multilingual t<span class="highlight-green">r</span>aining for <span class="highlight-red">c</span>r<span class="highlight-yellow">o</span>ss-lingual enhancement LLM.
</p>

<p>
We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM.
Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.
</p>
    <div class="project-overview">
        <img src="assets/images/marco_fig_init.png" alt="Model Performance Comparison" class="content-image">
        <p>Comparison of English-centric performance vs Multilingual performance on MMMLU and Flores. Our Marco-LLM demonstrates strong performance on both dimensions.</p>
    </div>

    <div class="project-overview">
        <img src="assets/images/Marco_Figure.png" alt="Model Overview" class="content-image">
        <p>An overview of the training and evaluation paradigm of our Marco-LLM, we conducted massive multilingual continual pre-training, multilingual supervised finetuning and preference alignment. We further perform extensive evaluation on multilingual benchmarks to validate the efficacy of our Marco-LLM.</p>
    </div>
</section>

<section id="highlights">
    <h2>Highlights</h2>
    <ul class="highlights-list">
        <li>We compile and curate a large-scale multilingual dataset tailored for low-resource languages, enhancing the diversity and richness of training data.</li>
        <li>We perform massive multilingual continual pre-training and post-training on the Qwen2 model to develop Marco-LLM, a multilingual LLM that substantially improves performance on low-resource language tasks.</li>
        <li>We conduct comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, AGIEval, multilingual MT-bench, etc, demonstrating that Marco-LLM outperforms state-of-the-art models in multilingual settings.</li>
    </ul>
</section>

<section id="training">
    <h2>Experimental Results</h2>
    
    <h3>Stage 1: Continual Pre-training</h3>
    <div class="training-stage">
        <img src="assets/images/ct_data.png" alt="Pre-training Data Distribution" class="content-image">
    </div>
    <div class="training-stage">
        <img src="assets/images/pretraining_data.png" alt="Pre-training Data Distribution" class="content-image">
    </div>
    <div class="training-stage">
        <img src="assets/images/ct_results.png" alt="Pre-training Results" class="content-image">
    </div>
    <h3>Stage 2: Multilingual Supervised Fine-tuning</h3>
    <div class="training-stage">
        <img src="assets/images/sft_results.png" alt="SFT Results" class="content-image">
    </div>    
    <div class="training-stage">
        <img src="assets/images/marco_mt_results.png" alt="Marco-MT Results" class="content-image">
    </div>   
    <div class="training-stage">
        <img src="assets/images/any2any.png" alt="Any2Any Results" class="content-image">
    </div>   
    <h3>Stage 3: Multilingual DPO</h3>
    <div class="training-stage">
        <img src="assets/images/dpo_results.png" alt="DPO Results" class="content-image">
    </div>    
</section>

<!-- <section id="results">
    <h2>Experimental Results</h2>
    <div class="results-container">
        <canvas id="resultsChart"></canvas>
        <table id="resultsTable">
            <thead>
                <tr>
                    <th>Metric</th>
                    <th>Our Model</th>
                    <th>Baseline</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>BLEU Score</td>
                    <td>42.3</td>
                    <td>38.7</td>
                </tr>
            </tbody>
        </table>
    </div>
</section> -->


