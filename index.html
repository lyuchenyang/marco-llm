---
layout: default
title: Multilingual LLM Project
---

<section id="introduction">
    <h1>Marco-LLM: Bridging Languages via Massive
Multilingual Training for Cross-Lingual Enhancement</h1>    
    <div class="project-overview">
        <img src="assets/images/marco_fig_init.png" alt="Model Performance Comparison" class="content-image">
        <p>Comparison of English-centric performance vs Multilingual performance on MMMLU and Flores. Our Marco-LLM demonstrates strong performance on both dimensions.</p>
    </div>

    <div class="project-overview">
        <img src="assets/images/Marco_Figure.png" alt="Model Overview" class="content-image">
        <p>An overview of the training and evaluation paradigm of our Marco-LLM, we conducted massive multilingual continual pre-training, multilingual supervised finetuning and preference alignment. We further perform extensive evaluation on multilingual benchmarks to validate the efficacy of our Marco-LLM.</p>
    </div>
</section>

<section id="highlights">
    <h2>Highlights</h2>
    <ul class="highlights-list">
        <li>We compile and curate a large-scale multilingual dataset tailored for low-resource languages, enhancing the diversity and richness of training data.</li>
        <li>We perform massive multilingual continual pre-training and post-training on the Qwen2 model to develop Marco-LLM, a multilingual LLM that substantially improves performance on low-resource language tasks.</li>
        <li>We conduct comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, AGIEval, multilingual MT-bench, etc, demonstrating that Marco-LLM outperforms state-of-the-art models in multilingual settings.</li>
    </ul>
</section>

<section id="training">
    <h2>Experimental Results</h2>
    
    <h3>Stage 1: Continual Pre-training</h3>
    <div class="training-stage">
        <img src="assets/images/ct_data.png" alt="Pre-training Data Distribution" class="content-image">
    </div>
    <div class="training-stage">
        <img src="assets/images/pretraining_data.jp2" alt="Pre-training Data Distribution" class="content-image">
    </div>
    <div class="training-stage">
        <img src="assets/images/ct_results" alt="Pre-training Results" class="content-image">
    </div>
    <h3>Stage 2: Multilingual Supervised Fine-tuning</h3>
    <div class="training-stage">
        <img src="assets/images/sft_results.png" alt="SFT Results" class="content-image">
    </div>    
    <div class="training-stage">
        <img src="assets/images/marco_mt_results.png" alt="Marco-MT Results" class="content-image">
    </div>   
    <div class="training-stage">
        <img src="assets/images/any2any.png" alt="Any2Any Results" class="content-image">
    </div>   
    <h3>Stage 3: Multilingual DPO</h3>
    <div class="training-stage">
        <img src="assets/images/dpo_results.png" alt="DPO Results" class="content-image">
    </div>    
</section>

<!-- <section id="results">
    <h2>Experimental Results</h2>
    <div class="results-container">
        <canvas id="resultsChart"></canvas>
        <table id="resultsTable">
            <thead>
                <tr>
                    <th>Metric</th>
                    <th>Our Model</th>
                    <th>Baseline</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>BLEU Score</td>
                    <td>42.3</td>
                    <td>38.7</td>
                </tr>
            </tbody>
        </table>
    </div>
</section> -->


